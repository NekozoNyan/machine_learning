{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('titanic.csv')\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# 处理缺失值\n",
    "data['Age'].fillna(data['Age'].mean(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n",
    "\n",
    "# 转换分类特征\n",
    "label_encoder = {}\n",
    "for column in ['Sex', 'Embarked']:\n",
    "   label_encoder[column] = {label:idx for idx, label in enumerate(data[column].unique())}\n",
    "   data[column] = data[column].map(label_encoder[column])\n",
    "   \n",
    "x = data[features].values\n",
    "y = data[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息熵\n",
    "def entropy(data):\n",
    "    label_cnts = Counter(data)\n",
    "    total_cnt = len(data)\n",
    "    return -sum(\n",
    "        (count / total_cnt) * math.log2(count / total_cnt) for count in label_cnts\n",
    "    )\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "def split_ds(data, labels, axis, value):\n",
    "    ret_ds = []\n",
    "    ret_label = []\n",
    "    for row, label in zip(data, labels):\n",
    "        if row[axis] == value:\n",
    "            reduced_row = list(row[:axis]) + list(row[axis + 1 :])\n",
    "            ret_ds.append(reduced_row)\n",
    "            ret_label.append(label)\n",
    "    return ret_ds, ret_label\n",
    "\n",
    "\n",
    "# 选择最佳划分特征\n",
    "def choose_best_features(data, lbls):\n",
    "    num_features = len(data[0])\n",
    "    base_entropy = entropy(lbls)\n",
    "    best_info_gain = 0.0\n",
    "    best_feature = -1\n",
    "\n",
    "    for i in range(num_features):\n",
    "        unique_vals = set(row[i] for row in data)\n",
    "        new_entropy = sum(\n",
    "            (len(subset) / len(lbls)) * entropy(subset_labels)\n",
    "            for val in unique_vals\n",
    "            for subset, subset_labels in [split_ds(data, lbls, i, val)]\n",
    "        )\n",
    "\n",
    "        info_gain = base_entropy - new_entropy\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_feature = i\n",
    "    return best_feature, best_info_gain\n",
    "\n",
    "\n",
    "# 多数表决\n",
    "def majority_cnt(class_list):\n",
    "    return Counter(class_list).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "# 创建决策树\n",
    "def create_tree(\n",
    "    data, labels, feature_names, max_depth=None, min_info_gain=0.01, depth=0\n",
    "):\n",
    "    class_list = labels\n",
    "    if class_list.count(class_list[0]) == len(class_list):\n",
    "        return class_list[0]\n",
    "    if len(data[0]) == 0:\n",
    "        return majority_cnt(class_list)\n",
    "    if max_depth is not None and depth >= max_depth:\n",
    "        return majority_cnt(class_list)\n",
    "\n",
    "    best_feature, best_info_gain = choose_best_features(data, labels)\n",
    "    if best_info_gain < min_info_gain:\n",
    "        return majority_cnt(class_list)\n",
    "\n",
    "    best_feature_name = feature_names[best_feature]\n",
    "    tree = {best_feature_name: {}}\n",
    "    unique_vals = set(row[best_feature] for row in data)\n",
    "    for value in unique_vals:\n",
    "        sub_feature_names = (\n",
    "            feature_names[:best_feature] + feature_names[best_feature + 1 :]\n",
    "        )\n",
    "        subset, subset_labels = split_ds(data, labels, best_feature, value)\n",
    "        tree[best_feature_name][value] = create_tree(\n",
    "            subset,\n",
    "            subset_labels,\n",
    "            sub_feature_names,\n",
    "            max_depth,\n",
    "            min_info_gain,\n",
    "            depth + 1,\n",
    "        )\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "# 后剪枝\n",
    "def post_prune(tree, validation_data, validation_labels, feature_names):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "\n",
    "    root = next(iter(tree))\n",
    "    subtrees = tree[root]\n",
    "    feature_index = feature_names.index(root)\n",
    "    for key in subtrees:\n",
    "        if isinstance(subtrees[key], dict):\n",
    "            subtrees[key] = post_prune(\n",
    "                subtrees[key], validation_data, validation_labels, feature_names\n",
    "            )\n",
    "\n",
    "    # 计算未剪枝误差\n",
    "    unprune_error = sum(\n",
    "        predict(tree, feature_names, sample) != label\n",
    "        for sample, label in zip(validation_data, validation_labels)\n",
    "    )\n",
    "    \n",
    "    # 计算剪枝后误差\n",
    "    majority_class = majority_cnt(validation_labels)\n",
    "    pruned_err = sum(majority_class != label for label in validation_labels)\n",
    "    \n",
    "    if pruned_err <= unprune_error:\n",
    "        return majority_class\n",
    "    else:\n",
    "        return tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
