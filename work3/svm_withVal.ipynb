{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集（无用）\n",
    "def data_split(data, test_size):\n",
    "    trn_data = {}\n",
    "    val_data = {}\n",
    "\n",
    "    for label, data in data.items():\n",
    "        lens = len(data[label])\n",
    "        print(lens)\n",
    "        trn_data[label] = data[: lens * (1 - test_size)]\n",
    "        val_data[label] = data[lens * (1 - test_size) :]\n",
    "    \n",
    "    return trn_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# 读取mat\n",
    "trn_mat = scipy.io.loadmat(\"data/train_data.mat\")\n",
    "trn_data = trn_mat[\"train\"]\n",
    "# print(len(trn_data))\n",
    "x_len = len(trn_data)\n",
    "\n",
    "# 标注数据label\n",
    "trn_label = [i + 1 for i in range(x_len)]\n",
    "# print(trn_label)\n",
    "\n",
    "# 将label与data组合为字典\n",
    "trn_dict = {}\n",
    "for i in range(x_len):\n",
    "    trn_dict[i+1] = trn_data[i]\n",
    "# print(len(trn_dict))\n",
    "print(type(trn_dict[1]))\n",
    "\n",
    "# 读取测试集\n",
    "val_mat = scipy.io.loadmat('data/test_data.mat')\n",
    "val_data = val_mat['test']\n",
    "val_data = val_data\n",
    "print(type(val_data[1]))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义SVM\n",
    "class LinearSVM:\n",
    "    # 定义学习率、正则化强度、迭代次数\n",
    "    def __init__(self, lr=1e-3, reg=1e-5, num_iter=1200):\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "    # 计算损失和梯度\n",
    "    def compute_loss_and_grad(self, x, y):\n",
    "        num_trn = x.shape[0] # ==2200\n",
    "        scores = x.dot(self.W)  # 获取全分数矩阵\n",
    "        print(scores.shape)\n",
    "        print(y.shape)\n",
    "        y -= 1\n",
    "        \n",
    "        correct_cls_scores = scores[np.arange(num_trn), y] # 提取正确类别的分数 \n",
    "        \n",
    "        margins = np.maximum(0, scores - correct_cls_scores[:, np.newaxis] + 1) # 计算损失边缘\n",
    "        margins[np.arange(num_trn), y] = 0  # 令正确分类损失为0\n",
    "        loss = np.sum(margins) / num_trn    # 计算损失\n",
    "        loss += self.reg * np.sum(self.W * self.W)    # 对损失进行正则化处理，防止过拟合\n",
    "        \n",
    "        margins[margins>0] = 1  # 将边缘损失矩阵大于零的部分设为1，方便计算梯度\n",
    "        margins[np.arange(num_trn), y] = -np.sum(margins, axis=1)   \n",
    "        dW = x.T.dot(margins) / num_trn # 计算权重矩阵的梯度取平均值，用于更新模型参数\n",
    "        dW += self.reg * self.W # 正则化梯度，用于惩罚过大的权重\n",
    "        \n",
    "        return loss, dW\n",
    "\n",
    "    # 训练\n",
    "    def fit(self, x, y):\n",
    "        num_trn, dim = x.shape\n",
    "        num_cls = np.max(y)\n",
    "        self.W = 0.001 * np.random.randn(dim, num_cls)  # 生成一个初始权重矩阵\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            loss, grad = self.compute_loss_and_grad(x, y)\n",
    "            self.W -= self.lr * grad    # 梯度下降法更新权重\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}/{self.num_iter}, Loss: {loss}\\ndW: {grad.shape}\")\n",
    "                \n",
    "    # 预测\n",
    "    def predict(self, x):\n",
    "        scores = x.dot(self.W)  # 利用权重进行预测\n",
    "        if scores.size == 0:\n",
    "            return []  # 如果输入序列为空，则返回空列表\n",
    "        return np.argmax(scores, axis=1)    # 输出得分最高的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 11, 28, 28)\n",
      "(2200,)\n",
      "[1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.98823529\n",
      " 0.97647059 0.99607843 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.71764706 0.40784314 0.93333333\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.68235294 0.10980392 0.81176471 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.80392157\n",
      " 0.10588235 0.74901961 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.8745098  0.14509804 0.58431373\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.96470588 0.29019608 0.36470588 0.98039216 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.50196078 0.18823529 0.90980392 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.71764706 0.10196078\n",
      " 0.77254902 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.83921569 0.11764706 0.70588235 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.87058824 0.23529412 0.75686275 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.98823529 0.88627451\n",
      " 0.97254902 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 收集并转换数据\n",
    "X = []\n",
    "Y = []\n",
    "X_trn = []\n",
    "X_val = []\n",
    "Y_trn = []\n",
    "Y_val = []\n",
    "for y, x in trn_dict.items():\n",
    "    X.append(x)\n",
    "    Y.append([y] * len(x))\n",
    "\n",
    "for label in range(200):\n",
    "    xt, xv, yt, yv = train_test_split(X[label], Y[label], test_size=0.25, random_state=42)\n",
    "    X_trn.append(xt)\n",
    "    X_val.append(xv)\n",
    "    Y_trn.append(yt)\n",
    "    Y_val.append(yv)\n",
    "\n",
    "X_trn = np.array(X_trn)\n",
    "Y_trn = np.array(Y_trn)\n",
    "print(X_trn.shape)\n",
    "\n",
    "# 重塑数据\n",
    "x_trn = X_trn.reshape(2200, 28 * 28).astype(np.float64)\n",
    "x_val = val_data.reshape(val_data.shape[0], -1).astype(np.float64)\n",
    "y_trn = Y_trn.reshape(2200, )\n",
    "print(y_trn.shape)\n",
    "x_trn /= 255.\n",
    "x_val /= 255.\n",
    "\n",
    "# # 计算并减去均值图像\n",
    "# mean_img = np.mean(x_trn, axis=0).astype(np.float64)\n",
    "# x_trn -= mean_img\n",
    "# x_val -= mean_img\n",
    "\n",
    "print(x_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 200)\n",
      "(2200,)\n",
      "Iteration 0/1200, Loss: 198.9641179966446\n",
      "dW: (784, 200)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "Iteration 100/1200, Loss: 199.66976344867646\n",
      "dW: (784, 200)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "(2200, 200)\n",
      "(2200,)\n",
      "Iteration 200/1200, Loss: 198.9963328242404\n",
      "dW: (784, 200)\n",
      "(2200, 200)\n",
      "(2200,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -201 is out of bounds for axis 1 with size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[204], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 训练 SVM\u001b[39;00m\n\u001b[1;32m      2\u001b[0m svm \u001b[38;5;241m=\u001b[39m LinearSVM()\n\u001b[0;32m----> 3\u001b[0m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[202], line 38\u001b[0m, in \u001b[0;36mLinearSVM.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(dim, num_cls)  \u001b[38;5;66;03m# 生成一个初始权重矩阵\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iter):\n\u001b[0;32m---> 38\u001b[0m     loss, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m grad    \u001b[38;5;66;03m# 梯度下降法更新权重\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[202], line 17\u001b[0m, in \u001b[0;36mLinearSVM.compute_loss_and_grad\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 17\u001b[0m correct_cls_scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_trn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# 提取正确类别的分数 \u001b[39;00m\n\u001b[1;32m     19\u001b[0m margins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, scores \u001b[38;5;241m-\u001b[39m correct_cls_scores[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 计算损失边缘\u001b[39;00m\n\u001b[1;32m     20\u001b[0m margins[np\u001b[38;5;241m.\u001b[39marange(num_trn), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# 令正确分类损失为0\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -201 is out of bounds for axis 1 with size 200"
     ]
    }
   ],
   "source": [
    "# 训练 SVM\n",
    "svm = LinearSVM()\n",
    "svm.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(800, 784)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [800, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_val\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m validations \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m---> 10\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_val))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/newSpace/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m~/newSpace/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/newSpace/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/newSpace/anaconda3/envs/pytorch/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [800, 200]"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "x_val = np.array(x_val)\n",
    "print(x_val.shape)\n",
    "predictions = svm.predict(x_val)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "X_val = X_val.reshape(800, 28 * 28)\n",
    "print(X_val.shape)\n",
    "validations = svm.predict(X_val)\n",
    "acc = accuracy_score(validations, Y_val)\n",
    "\n",
    "print(len(x_val))\n",
    "print(f\"acc: {acc}\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预测结果写入csv\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv('data/submission.csv', encoding='utf8')\n",
    "submission['预测结果'] = predictions\n",
    "submission.to_csv('data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
